{"cells":[{"cell_type":"markdown","source":["### Exploring social ties between GitHub collaborators and resulting repository popularity\n\nTracy Lo</br>\nStudent #: 998963028"],"metadata":{}},{"cell_type":"markdown","source":["#### Introduction\n\nThe research topic that has been chosen is a study on the social ties between GitHub collaborators and the resulting popularity of the projects where the collaborators have worked together in the past. In open source communities, there are various factors which may motivate one to participate in a project team – of these motivations, social ties and relationships are amongst the top reasons for project collaboration in open source projects. An investigation on the social ties between collaborators on Github and the impacts on the resulting repository popularity shall be explored."],"metadata":{}},{"cell_type":"markdown","source":["#### Revised Project Plan \n\nThe project plan for the remaining time period of the project has been revised based on the feedback gathered from V1:\n\n* Week 2 (V2):\n  * Create graphs - 6 hrs\n    * Developer-project network - 3 hrs \n    * Developer-developer network - 3 hrs\n  * Popularity Prediction - 4 hrs\n    * K-Means Implementation\n    * Model Evaluation (Silhouette)\n* Week 3 (V3): \n  * Make revisions - 2 hrs\n  * Full validation of data preparation - 4 hrs\n  * Evaluate results and review process - 4 hrs\n* Week 4:\n  * Create Final Report - 5 hrs\n\nDuring each phase, a base of 10 hours will be allocated for tasks. Week 4 will require less time (5 hrs) as the work should be complete, save for formatting of the final report. Additional buffer time (~2 hrs) will be added to each version in the event that any issues or unexpected changes that may arise."],"metadata":{}},{"cell_type":"markdown","source":["#### Project Plan \n\nThe project plan has been constructed following the tasks outlined in CRISP-DM and broken down into a four-week plan:\n\n* Week 1 (V1):\n  * Data Understanding: exploration, verification - 3.5 hrs\n    * Ingest data - 0.5 hr\n    * Describe and assess metadata - 3 hrs\n  * Data Preparation: selection, cleaning - 4.5 hrs\n    * Cleaning - 1.5 hr\n    * Transformations on Big Query - 3 hrs\n  * Create project plan - 2 hr\n    * Set up Databricks notebook - 1 hr\n    * Task breakdown and scheduling - 1 hr\n* Week 2 (V2):\n  * Make updates to project summary and plan based on instructor feedback - 6 hrs \n    * Tasks to be determined depending on feedback received\n  * Full validation of data preparation - 4 hrs\n* Week 3 (V3): \n  * Make revisions - 2 hrs\n  * Modelling: Build and assess model - 5 hrs\n    * Create graph to model developer social ties - 2.5 hrs\n    * Implement k-means algorithm on repository popularity - 2.5 hrs\n  * Evaluate results and review process - 3 hrs\n* Week 4:\n  * Create Final Report - 5 hrs\n\nDuring each phase, a base of 10 hours will be allocated for tasks. Week 4 will require less time (5 hrs) as the work should be complete, save for formatting of the final report. Additional buffer time (~2 hrs) will be added to each version in the event that any issues or unexpected changes that may arise."],"metadata":{}},{"cell_type":"markdown","source":["#### Data Understanding\n\n##### Ingestion and Description of Metadata \nAs part of the data understanding process, the data was ingested from GHTorrent on Google BigQuery. Once ingested, the individual tables were assessed for structure, granularity, accuracy, temporality, and scope.\n\nThe following tables from GHTorrent [6] on Google BigQuery were ingested. Data columns which are required by the scope of the project topic are bolded.\n* [ghtorrent-bq:ght.users]: This table describes all users on GitHub.\n  * **id**: user id\n  * **login**: user login\n  * company: if user belongs to a company, name of company - not required\n  * **created_at**: user's date of creation\n  * **type**: user or org (org = organizations)\n  * **fake**: boolean (fake users are users who appear as authors or commiters of commits)\n  * **deleted**: boolean (deleted users are users which GHTorrent cannot get details for)\n  * long: longitude value for location of user - not required\n  * lat: latitude value for location of user - not required\n  * country_code: user's country of origin - not required\n  * state: user's state of origin - not required\n  * city: user's city of origin - not required\n  * location: user's location of origin - not required\n* [ghtorrent-bq:ght.projects]: This table describes all projects on GitHub.\n  * **id**: project id\n  * url: project url - not required\n  * **owner_id**: user id of project owner\n  * **name**: project name\n  * description: project description - not required\n  * language: project language - not required\n  * **created_at**: date which project was created at\n  * forked_from: if project was forked from another project, o\n  * **deleted**: boolean (whether project is deleted from GitHub)\n  * **updated_at**: date which project was updated at\n* [ghtorrent-bq:ght.project_members]: This table describes all members (defined as users who have commit access) of specific projects.\n  * **repo_id**: project id\n  * **user_id**: user id\n  * **created_at**: date which user joined project; only accurate if GHTorrent has recorded the event else the date is replaced with the creation date of the user or project\n  * dont_use: no description in GHTorrent schema - not required\n* [ghtorrent-bq:ght.commits]: This table describes all unique commits made.\n  * **id**: commit id\n  * sha: global commit id - not required\n  * author_id: commit author id - not required\n  * **committer_id**: commiter id\n  * **project_id**: project which commit was associated with; may differ from project id that commit was initially pushed to (requires project_commits)\n  * **created_at**: date which commit was made\n* [ghtorrent-bq:ght.project_commits]: This table describes all commits made for specific projects.\n  * **project_id**: project id \n  * **commit_id**: commit id \n* [ghtorrent-bq:ght.watchers]: This table describes all users who have watched (starred) projects.\n  * **repo_id**: project id\n  * **user_id**: user id\n  * **created_at**: date which user watched/starred project; only accurate if GHTorrent has recorded the event else the date is replaced with the creation date of the user or project\n* [ghtorrent-bq:ght_2018_04_01.pull_request]: This table describes all pull requests made.\n  * **id**: pull request id\n  * head_repo_id: repo id that pull request is from\n  * **base_repo_id**: repo id that pull request goes to\n  * head_commit_id: commit id that pull request is from\n  * base_commit_id: commiit id that pull request goes to\n  * **pullreq_id**: pull request id on Github\n  * **intra_branch**: boolean (signifies whether head and base repos are the same)\n* [ghtorrent-bq:ght_2018_04_01.pull_request_history]: This table describes all pull requests and their corresponding events.\n  * **id**: pull request id\n  * **pull_request_id**: pull request id on Github\n  * **created_at**: date which pull request was made\n  * **action**: action of event corresponding to entry (can be opened/closed/merged/reopened/synchronized)\n  * **actor_id**: user id of user who initiated pull request"],"metadata":{}},{"cell_type":"markdown","source":["#### Data Preparation\n\n##### Transformations (on BigQuery)\nThe tables above were transformed on BigQuery through intrarecord and interrecord structuring to create more meaningful tables for the purpose of the project topic.\n* Active Projects\n  * Only active projects will be considered for the scope of the project. According to Gousios [6], GHTorrent returns all projects, including those which have been deleted. For the purposes of the project, these deleted projects were filtered out. In addition, it was decided that the definition of \"active projects\" included only projects which had:\n    * commits within the last year -  this was achieved through the creation of a new \"active projects - commits\" table which utilized columns from the commits, projects, and project_commits tables, filtered by the project created_at field.\n    * pull requests within the last year - this was achieved through the creation of a new \"active projects - pull requests\" table which utilized columns from the pull_requests, projects, and pull_request_history tables, filtered by the project created_at field.\n* Project Members\n  * Antwerp et al. [3] creates a graph to model the developer network using a list of projects and the respective developers working on said projects. To determine the members of a specific project, specific columns chosen above are combined from the users, projects and project_members table to create a list of projects with the respective project owners and members. Users which are \"fake\" or \"deleted\" were removed from the dataset.\n* Project Stars on Weekly Basis\n  * To determine the popularity of a project, Borges et al. [4] investigates project popularity through the usage of the “star” functionality. According to Gousios [6], the watchers table provides the list of users who have starrted/watched a project. The watchers table was transformed such that each star action was grouped into the respective projects and assigned a created_at week number. The total number of star actions were then summed to retrieve a weekly count of each project's stars.  \n  \n##### Cleaning\n* Formatting of all created_at columns is YYYY-MM-DD HH:MI:SS.SSSSSSSS UTC, which does not adhere with the correct timestamp format (YYYY-MM-DD HH:MI:SS) and therefore will need to be modified\n* Invalid values may occur for:\n  * created_at timestamps, which is accurate only if GHTorrent has recorded the event - else the date is replaced with the creation date of the user or project. Comparisons between project and member created_at dates will be compared to watchers and project_members created_at dates - if they are the same value, possibly exclude them from the dataset. [7]\n* No missing or null values were found in the transformed tables"],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Import required libraries\nimport java.sql.Timestamp\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\n\n//Patching string class with new functions that have a default value if conversion to another type fails\nimplicit class StringConversion(val s: String) {\ndef toTypeOrElse[T](convert: String=>T, defaultVal: T) = try {\n    convert(s)\n  } catch {\n    case _: Throwable => defaultVal\n  }\n  \n  def toIntOrElse(defaultVal: Int = 0) = toTypeOrElse[Int](_.toInt, defaultVal)\n  def toDoubleOrElse(defaultVal: Double = 0D) = toTypeOrElse[Double](_.toDouble, defaultVal)\n  def toDateOrElse(defaultVal: java.sql.Timestamp = java.sql.Timestamp.valueOf(\"1970-01-01 00:00:00\")) = toTypeOrElse[java.sql.Timestamp](java.sql.Timestamp.valueOf(_), defaultVal)\n}\n\n// Load CSV files\nval activeprojects_prRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1-CqOeSGp98VHOXQhS3oGOOq19fIBUELw\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"project_id\")\n\nval activeprojects_commitsRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1RHb6yDmbdwEXFV1gOCZEwCM0R45YhzVD\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"project_id\")\n\nval activeprojects_allRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1RWNY18NUPZqHey2cMkF4_4OSxciNXYYy\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"project_id\")\n\nval duration_commitsRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1RU7SKoaPh5ahupzvtdBHhPRdvSzsz17x\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"repo_id\")\n\nval duration_prRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1XE6idJDlC0jdpQjgxTkzGbVe0ku36C8x\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"repo_id\")\n\nval watchersRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=12hkHE4MuKIJKO3e4TBbkTptk_sfit6wr\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"repo_id\")\n\nval topproj_watchersRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1gSR25-CDD8wBidcgU3Gx1PvxAXlaQUfS\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"repo_id\")\n\n//val allwatchersRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1dhNivmsOrKTUPUpDQXQf0UU78L7pXS5d\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"repo_id\")\n\nval projectsdurationRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=1WPL-Bv1qZR_EB4eLvtBZCHTmke3DWQyi\"), Charset.forName(\"utf8\")).split(\"\\n\")).map(line => line.split(\",\", -1).map(_.trim)).filter(line => line(0) != \"project_id\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### SQL Queries (Run on BigQuery)"],"metadata":{}},{"cell_type":"markdown","source":["According to Rattenbury et al. [7], data should be assessed for whether or not it is \"stale\" as part of the temporality assessment. In terms of Github, \"stale\" data can be represented by repositories which are inactive (no \"activity\" defined by commits or pull requests). In a study on mining Github data, Kalliamvakou et. al. [8] identified that most projects are inactive on Github. In order to ensure that the dataset used in the project will create meaningful results, the following SQL query was run on BigQuery to find all active projects with commits within the last year (entire query results in the \"[reliable-stage-199320:github.projects_activecommits_1yr]\" table which is imported as activeprojects_commitsRDD). This was done by joining the projects_commits and commits table to find the commits associated with each project. The resulting table was then joined with the projects table to get more details (i.e. project name, project owner id, project creation date, project updated date) of the projects which had commits to it. The final table was then filtered to show only projects which had commits created within the past year in order to show \"active\" projects. However, as the table displays all commits made to a project within the past year, users which made multiple commits would appear multiple times. As part of the data scope assessment, a new column of \"ranking\" was created to sort all commits by their date in descending order. In order to get distinct users who made commits to the projects within the past year, the \"ranking\" column was populated using window functions, were used to rank the commits by the commit date and only the most recent commit which a user made was taken."],"metadata":{}},{"cell_type":"code","source":["'''\nActive Projects with Commits in the Past Year: following query resulted in the [reliable-stage-199320:github.projects_activecommits_1yr] table\n\nSELECT\n  project_id,\n  project_owner_id,\n  project_name,\n  project_created_date,\n  last_commit_date,\n  ranking\nFROM (\n  SELECT\n    c.project_id AS project_id,\n    p.owner_id AS project_owner_id,\n    p.name AS project_name,\n    p.created_at AS project_created_date,\n    c.created_at AS last_commit_date,\n    ROW_NUMBER() OVER (PARTITION BY project_id ORDER BY last_commit_date DESC) AS ranking\n  FROM\n    [ghtorrent-bq.ght.commits] AS c\n  JOIN\n    [ghtorrent-bq.ght.projects] AS p\n  ON\n    p.id = c.project_id\n  WHERE\n    (c.created_at BETWEEN DATE_ADD(CURRENT_TIMESTAMP(), -1, 'YEAR')\n      AND CURRENT_TIMESTAMP()) )\nWHERE\n  ranking = 1\nORDER BY\n  last_commit_date desc\n'''"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Kalliamvakou et. al. [8] study on mining Github data also identified that most projects have very little commits which meant that the first table which returned active projects based soley on commits would not be enough. Thus, pull requests were also incorporated to create a full list of active projects (entire query results in the \"[reliable-stage-199320:github.projects_activepr_1yr]\" table which is imported as activeprojects_prRDD). Active projects with pull requests within the past year were found by joining the pull_request and pull_request_history tables to determine the pull requests associated with each project. The resulting table was then joined with the projects table to get more details (i.e. project name, project owner id, project creation date, project updated date) of the projects which had pull requests to it. The final table was then filtered to show only projects which had pull requests created within the past year in order to show \"active\" projects. However, as the table displays all pull requests made to a project within the past year, users which made multiple pull requests would appear multiple times. As part of the data scope assessment, a new column of \"ranking\" was created to sort all PRs by their date in descending order. In order to get distinct users who made PRs to the projects within the past year, the \"ranking\" column was populated using window functions, were used to rank the PRs by the PR date and only the most recent PR which a user made was taken."],"metadata":{}},{"cell_type":"code","source":["'''\nActive Projects with Pull Requests in the Past Year: following query resulted in the [reliable-stage-199320:github.projects_activepr_1yr] table.\n\nSELECT\n  project_id,\n  project_name,\n  project_created_date,\n  last_pr_date,\n  ranking\nFROM (\n  SELECT\n    pr_prh.pullreq_id AS pullreq_id,\n    p.owner_id AS project_owner_id,\n    p.id as project_id,\n    p.name AS project_name,\n    p.created_at AS project_created_date,\n    pr_prh.created_at AS last_pr_date,\n    ROW_NUMBER() OVER (PARTITION BY p.id ORDER BY last_pr_date DESC) AS ranking\n  FROM\n    [ghtorrent-bq.ght.projects] AS p\n  JOIN (\n    SELECT\n      pr.id AS id,\n      pr.base_repo_id AS base_repo_id,\n      pr.pullreq_id AS pullreq_id,\n      prh.pull_request_id AS pull_request_id,\n      prh.action AS action,\n      prh.created_at AS created_at\n    FROM\n      [ghtorrent-bq.ght_2018_04_01.pull_request_history] AS prh\n    JOIN\n      [ghtorrent-bq.ght_2018_04_01.pull_requests] AS pr\n    ON\n      prh.pull_request_id = pr.id\n    WHERE\n      action = 'merged' and (prh.created_at BETWEEN DATE_ADD(CURRENT_TIMESTAMP(), -1, 'YEAR')\n      AND CURRENT_TIMESTAMP()) ) AS pr_prh\n  ON\n    p.id = pr_prh.base_repo_id)\nWHERE\n  ranking = 1\nORDER BY\n  last_pr_date DESC\n'''"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Once the list of projects which had commits and pull requests made within the past year were retrieved, a new table \"[reliable-stage-199320:active_projects_commitsprscombined]\" (imported as activeprojects_allRDD) was created to combine the projects from both lists."],"metadata":{}},{"cell_type":"code","source":["'''\nAll active projects (pull requests and commits within past year): following query resulted in the [reliable-stage-199320:active_projects_commitsprscombined] table.\n\nSELECT \n  project_id, \n  project_name, \n  project_created_date \nFROM \n  [reliable-stage-199320:github.projects_activepr_1yr], \n  [reliable-stage-199320:github.projects_activecommits_1yr]\n'''"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Antwerp et. al. [8] study on open source developer relationships began with gathering the required data to construct a developer-developer network. In order to construct the network, a list of projects and the developers who worked on said projects were required. In addition, the dates of each developer's first and last contribution to the project were required. This would be used to calculate whether or not developers collaborated - if their first and last contribution date fell within another developer's first and last contribution date, then the two would have worked together at some point. To gather this data in the Github dataset, the commits and pull requests were looked at separately. \n\nTo begin, the commits and users tables were joined to determine the user details of each committer and the first and last commit the particular user made (using max and min on window functions) within the past year. From there, the resulting table was joined with the list of active projects (\"[reliable-stage-199320:active_projects_commitsprscombined]\" table above) in order to determine which users had made commits to the list of active projects which were identified in the previous step. The final \"[reliable-stage-199320:github.usercommits_activeprojects]\" table query is shown below, and is imported as duration_commitsRDD."],"metadata":{}},{"cell_type":"code","source":["'''\nAll users who made commits (with first and last commit dates) on active projects: following query resulted in the [reliable-stage-199320:github.usercommits_activeprojects] table.\n\nSELECT\n  ap.project_id AS project_id,\n  uc.member_id AS member_id,\n  uc.member_login AS member_login,\n  uc.latest_commit AS latest_commit,\n  uc.oldest_commit AS oldest_commit\nFROM (\n  SELECT\n    u.id AS member_id,\n    u.login AS member_login,\n    c.project_id AS project_id,\n    c.created_at,\n    MAX(c.created_at) OVER (PARTITION BY project_id, member_id, member_login) AS latest_commit,\n    MIN(c.created_at) OVER (PARTITION BY project_id, member_id, member_login) AS oldest_commit\n  FROM\n    [ghtorrent-bq:ght.commits] AS c\n  JOIN\n    [ghtorrent-bq:ght.users] AS u\n  ON\n    c.author_id = u.id\n  WHERE\n    u.fake = FALSE\n    AND u.deleted = FALSE\n    AND (c.created_at BETWEEN DATE_ADD(CURRENT_TIMESTAMP(), -1, 'YEAR')\n      AND CURRENT_TIMESTAMP())\n  GROUP BY\n    member_id,\n    member_login,\n    project_id,\n    c.created_at\n  ORDER BY\n    project_id) AS uc\nJOIN\n  [reliable-stage-199320:github.active_projects_commitsprscombined] AS ap\nON\n  ap.project_id = uc.project_id\nGROUP BY \n  project_id, \n  member_id, \n  member_login, \n  latest_commit, \n  oldest_commit\n'''"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Next, the pull requests, pull requests history and users tables were joined to determine the user details of each pull request author and the first and last pull request the particular user made (using max and min on window functions) within the past year. From there, the resulting table was joined with the list of active projects (\"[reliable-stage-199320:active_projects_commitsprscombined]\" table above) in order to determine which users had made pull requests to the list of active projects which were identified in the previous step. The final \"[reliable-stage-199320:github.userpr_activeprojects]\" table query is shown below, and is imported as duration_prRDD."],"metadata":{}},{"cell_type":"code","source":["'''\nAll users who made pull requests (with first and last pull request dates) on active projects:  Following query resulted in the [reliable-stage-199320:github.userpr_activeprojects] table.\n\nSELECT\n  ap.project_id AS project_id,\n  upr.member_id AS member_id,\n  upr.member_login AS member_login,\n  upr.latest_pull_request AS latest_pr,\n  upr.oldest_pull_request AS oldest_pr\nFROM (\n  SELECT\n    u.id AS member_id,\n    u.login AS member_login,\n    pr_prh.base_repo_id AS project_id,\n    pr_prh.created_at,\n    MAX(pr_prh.created_at) OVER (PARTITION BY project_id, member_id, member_login) AS latest_pull_request,\n    MIN(pr_prh.created_at) OVER (PARTITION BY project_id, member_id, member_login) AS oldest_pull_request\n  FROM (\n    SELECT\n      pr.id AS id,\n      pr.base_repo_id AS base_repo_id,\n      pr.pullreq_id AS pullreq_id,\n      prh.pull_request_id AS pull_request_id,\n      prh.action AS action,\n      prh.actor_id as actor_id,\n      prh.created_at AS created_at\n    FROM\n      [ghtorrent-bq:ght_2018_04_01.pull_request_history] AS prh\n    JOIN\n      [ghtorrent-bq:ght_2018_04_01.pull_requests] AS pr\n    ON\n      prh.pull_request_id = pr.id) as pr_prh\n  JOIN\n    [ghtorrent-bq:ght.users] AS u\n  ON\n    pr_prh.actor_id = u.id\n  WHERE\n    u.fake = FALSE\n    AND u.deleted = FALSE\n    AND (pr_prh.created_at BETWEEN DATE_ADD(CURRENT_TIMESTAMP(), -1, 'YEAR')\n      AND CURRENT_TIMESTAMP())\n  GROUP BY\n    member_id,\n    member_login,\n    project_id,\n    pr_prh.created_at\n) AS upr\nJOIN\n  [reliable-stage-199320:github.active_projects_commitsprscombined] AS ap\nON\n  ap.project_id = upr.project_id\nGROUP BY\n  project_id,\n  member_id,\n  member_login,\n  latest_pr,\n  oldest_pr\n'''"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["The separate tables of users who have created commits and pull requests are then combined in order to get the full list of users who have create commits, pull requests, or both to any of the active projects (projects which have pull requests or commits within the past year). The resulting table query is shown below and is imported as projectsdurationRDD."],"metadata":{}},{"cell_type":"code","source":["'''\nFollowing query resulted in table to display list of all members and their latest/oldest commit or PR within active projects (commits or PRs within last year)\n\nSELECT\n  project_id,\n  member_id,\n  member_login,\n  latest_pr,\n  oldest_pr,\n  latest_commit,\n  oldest_commit\nFROM\n  [reliable-stage-199320:github.userpr_activeprojects],\n  [reliable-stage-199320:github.usercommits_activeprojects]\nORDER BY\n  project_id'''"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["In order to perform the project popularity analysis, Borges et. al [4] requires the number of stars acquired per project per week for a 52 week period. The following query shown below was run to gather the number of stars acquired on a weekly basis. To begin, the watchers table which lists all possible projects is joined with the \"[reliable-stage-199320:github.active_projects_commitsprscombined]\" table to return the list of active projects based on the last commit or PR within the past year. Next, the week function was used to extract the week from the created_at dates that stored the date of the star event. Next, all star events for a project within the particular week of the year were summed in order to get the total number of star events for a given week. The resulting table is imported as watchersweekly."],"metadata":{}},{"cell_type":"code","source":["'''\nWatchers per Week - ran on BigQuery\n\nSELECT\n  t.repo_id AS repo_id,\n  t.repo_name as repo_name,\n  COUNT(*) AS number_watchers,\n  t.week_number AS week_number\nFROM (\n  SELECT\n    w.repo_id as repo_id,\n    ap.project_name as repo_name,\n    w.created_at as created_at,\n    WEEK(w.created_at) AS week_number\n  FROM\n    [ghtorrent-bq:ght.watchers] as w\n    join\n    [reliable-stage-199320:github.active_projects_commitsprscombined] as ap\n    on w.repo_id = ap.project_id\n  ORDER BY\n    repo_id,\n    repo_name,\n    week_number\n  ) AS t\nGROUP BY\n  repo_id,\n  repo_name,\n  week_number\nORDER BY\n  repo_id,\n  repo_name,\n  week_number\n'''"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### activeprojects_prRDD - Exploratory Analysis\n\nTo begin the exploratory analysis portion, the csv files were loaded in a previous step. Next, the csv files were cleaned according to the data preparation process as mentioned - as part of the data accuracy assessment, date times should be adjusted to a standard time zone and in the correct format. The dataset was originally in UTC format already; however, the timestamps all have a \"UTC\" string at the end and therefore will be removed in the cleanActiveProjectsPR function."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Define schema\ncase class activeProjectsPR(                 \n   project_id: Int,\n   project_name: String,\n   project_created_at: java.sql.Timestamp,\n   last_pr_date: java.sql.Timestamp,\n   ranking: Int\n  )\n\n// Map data to schema\ndef cleanActiveProjectsPR(row:Array[String]):activeProjectsPR = {\nreturn activeProjectsPR(\n    row(0).toIntOrElse(),\n    row(1),\n    Timestamp.valueOf(row(2).replace(\"UTC\", \"\")),\n    Timestamp.valueOf(row(3).replace(\"UTC\", \"\")),\n    row(4).toIntOrElse()\n  )\n}\n\n// Create sql table \nval activeprojectspr = activeprojects_prRDD.map(r => cleanActiveProjectsPR(r)).toDF()\n// register this data as an SQL table\nactiveprojectspr.createOrReplaceTempView(\"activeprojectspr\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Place data in dataframe and observe\ndf_pr = sqlContext.table(\"activeprojectspr\")\ndisplay(df_pr)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["As part of the data temporality assessment, the dates of which the last pull request of each project was observed. From BigQuery, the table was from April 1 2018, which can be taken as the date that the dataset was collected. The results below show that the records temporal range is a very small window, between March 28 - March 31 2018. The data can be considered \"fresh\" (as opposed to \"stale\") since the last PR date (which can be used to represent the last time that the project has been modified) is within the year limit which was set as an \"active\" project requirement."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* A count of active projects in each month shows that all PRs within the 3rd month (March). */\n\nSELECT month(last_pr_date) as month_of_last_pr,\n       count(project_id) as active_projects   \nFROM activeprojectspr\nGROUP BY month(last_pr_date)\nORDER BY month_of_last_pr\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\n/* A count of active projects for each day shows that the PRs are all within the last 4 days of the month (28th - 31st). */\n\nSELECT day(last_pr_date) as day_of_last_pr,\n       count(project_id) as active_projects   \nFROM activeprojectspr\nGROUP BY day(last_pr_date)\nORDER BY day_of_last_pr\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### activeprojects_commitsRDD - Exploratory Analysis\n\nTo begin the exploratory analysis portion, the csv files were loaded in a previous step. Next, the csv files were cleaned according to the data preparation process as mentioned - as part of the data accuracy assessment, date times should be adjusted to a standard time zone and in the correct format. The dataset was originally in UTC format already; however, the timestamps all have a \"UTC\" string at the end and therefore will be removed in the cleanActiveProjectsCommits function."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Define schema\ncase class activeProjectsCommits(                 \n   project_id: Int,\n   project_name: String,\n   project_owner_id: Int,\n   project_owner_login: String,\n   project_created_at: java.sql.Timestamp,\n   last_commit_date: java.sql.Timestamp,\n   ranking: Int\n  )\n\n// Map data to schema\ndef cleanActiveProjectsCommits(row:Array[String]):activeProjectsCommits = {\nreturn activeProjectsCommits(\n    row(0).toIntOrElse(),\n    row(1),\n    row(2).toIntOrElse(),\n    row(3),\n    Timestamp.valueOf(row(4).replace(\"UTC\", \"\")),\n    Timestamp.valueOf(row(5).replace(\"UTC\", \"\")),\n    row(6).toIntOrElse()\n  )\n}\n\n// Create sql table \nval activeprojectscommits = activeprojects_commitsRDD.map(r => cleanActiveProjectsCommits(r)).toDF()\n// register this data as an SQL table\nactiveprojectscommits.createOrReplaceTempView(\"activeprojectscommits\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Place data in dataframe and observe\ndf_commits = sqlContext.table(\"activeprojectscommits\")\ndisplay(df_commits)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["As part of the data temporality assessment, the dates of which the last commit of each project was observed. The results below show that the records temporal range is a very large, spanning the entire year. This is a sharp contrast with the pull requests dataset, which was concentrated between the 4 days of April 28-31st 2018. However, the data can be still considered \"fresh\" (as opposed to \"stale\") since the last PR date (which can be used to represent the last time that the project has been modified) is within the year limit which was set as an \"active\" project requirement."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* A count of active projects in each month for 12 months shows that active projects are evenly distributed throughout the 1-year period. */\n\nSELECT month(last_commit_date) as month_of_last_commit,\n       count(project_id) as active_projects   \nFROM activeprojectscommits\nGROUP BY month(last_commit_date)\nORDER BY month_of_last_commit\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["As part of the data exploration process, data scope can be evaluated by observing the characteristics of the dataset. In the cell below, an analysis will be performed on the activeprojectcommits dataset in order to determine the distribution of projects a user owns. As shown in the graph, the majority of users (93%) own 1 project, with a small percentage (7%) of users owning 2 projects."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* Distribution of projects a user owns. Majority of users own 1 project, with 5% of users owning 2 projects. */\n\nSELECT project_owner_id, \n       count(distinct(project_id)) as projects\nFROM activeprojectscommits\nGROUP BY project_owner_id"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["#### Watchers/Starred Events - Exploratory Analysis\n\nTo begin the exploratory analysis portion, the csv files were loaded in a previous step. Next, the csv files were cleaned and imported into a dataframe."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Define schema\ncase class watchersWeekly(                 \n   repo_id: Int,\n   repo_name: String,\n   number_watchers: Int, \n   week_number: Int\n  )\n\n// Map data to schema\ndef cleanWatchersWeekly(row:Array[String]):watchersWeekly = {\nreturn watchersWeekly(\n    row(0).toIntOrElse(),\n    row(1),\n    row(2).toIntOrElse(),\n    row(3).toIntOrElse()\n  )\n}\n\n// Create sql table \nval watchersweekly = watchersRDD.map(r => cleanWatchersWeekly(r)).toDF()\n// register this data as an SQL table\nwatchersweekly.createOrReplaceTempView(\"watchersweekly\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Place data in dataframe and observe\ndf = sqlContext.table(\"watchersweekly\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["In order to perform the popularity analysis using the star events of a project, Borges et al. [4] only considers the stars gained in the last 52 weeks of each repository and removes all repositories with less than 52 weeks. First, a count of the total number of weeks each repository has is performed in the first query. As shown, there are repositories which have less than 53 weeks."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* Find the number of weeks each project has star records for */\n\nselect t.repo_id, t.repo_id, t.total_weeks from (select repo_id, repo_name, count(week_number) as total_weeks\nfrom watchersweekly\ngroup by repo_id, repo_name) as t"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["This query follows the assessment criteria for data scope, which includes the observation of consistency of record fields. Since there are inconsistencies in the record fields (some repositories have 53 weeks worth of star counts; other have less), it is important to remove the inconsistent records (repositories with less than 53 weeks worth of data) to retain a consistent dataset. The next query shown below is performed to remove all repositories which have less than 53 weeks of star events recorded."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* Find all projects which have star records for 53 weeks (full year record) */\n\nselect t.repo_id, t.repo_id, t.total_weeks from (select repo_id, repo_name, count(week_number) as total_weeks\nfrom watchersweekly\ngroup by repo_id, repo_name) as t\nwhere t.total_weeks = 53"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Once the list of repositories which have consistent records of star events are retrieved, the counts of each week's star events are retrieved by joining the results of said table with the watchersweekly table, as shown below. A line plot displaying each project's total star events (y-axis) for each week of the year (x-axis) is shown below the query. It is observed that there are several repositories which have spikes in the total star events - including those with repo_id of 37, 528, and 540."],"metadata":{}},{"cell_type":"code","source":["%sql\n/* Find all projects which have star records for 53 weeks (full year record) */\n\nselect w.repo_id, w.repo_name, w.number_watchers, w.week_number\nfrom\nwatchersweekly as w\njoin (select t.repo_id, t.repo_id, t.total_weeks from (select repo_id, repo_name, count(week_number) as total_weeks\nfrom watchersweekly\ngroup by repo_id, repo_name) as t\nwhere t.total_weeks = 53) as fr\non w.repo_id = fr.repo_id"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%scala\n\n// register this data as an SQL table\nval fr_watchersDF = sqlContext.sql(\"select w.repo_id, w.repo_name, w.number_watchers, w.week_number from watchersweekly as w join (select t.repo_id, t.repo_id, t.total_weeks from (select repo_id, repo_name, count(week_number) as total_weeks from watchersweekly group by repo_id, repo_name) as t where t.total_weeks = 53) as fr on w.repo_id = fr.repo_id\")\nfr_watchersDF.createOrReplaceTempView(\"fr_watchers\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["#### Project Members and their latest/oldest Commit or PR within Active Projects - Exploratory Analysis\n\nThe query which was run in order to return all project members and their latest/oldest commit or PR was imported as projectdurationRDD and shown in the previous steps. The data preparation for the projectsdurationRDD including cleaning and parsing the data into a dataframe are performed in the following cell. Since the projects do not haveall fields for latest_pr, oldest_pr, latest_commit, and oldest_commit populated, these columns were first parsed as strings, then converted to timestamps, as shown below. The resulting dataframe \"activeprojectsduration_temp\" is shown below."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.{col, unix_timestamp}\n\n// Define schema\ncase class projectsDuration(                 \n   project_id: Int,\n   project_name: String,\n   member_id: Int,\n   member_login: String,\n   latest_pr: String,\n   oldest_pr: String,\n   latest_commit: String,\n   oldest_commit: String\n  )\n\n// Map data to schema\ndef cleanProjectsDuration(row:Array[String]):projectsDuration = {\nreturn projectsDuration(\n    row(0).toIntOrElse(),\n    row(1),\n    row(2).toIntOrElse(),\n    row(3),\n    row(4),\n    row(5),\n    row(6),\n    row(7)\n  )\n}\n\n// Create sql table \nval activeprojectsduration_initial = projectsdurationRDD.map(r => cleanProjectsDuration(r)).toDF()\n\nval updatedDF = activeprojectsduration_initial.withColumn(\"latest_pr\", unix_timestamp(col(\"latest_pr\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\nval updatedDF1 = updatedDF.withColumn(\"oldest_pr\", unix_timestamp(col(\"oldest_pr\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\nval updatedDF2 = updatedDF1.withColumn(\"latest_commit\", unix_timestamp(col(\"latest_commit\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\nval activeprojectsduration_temp = updatedDF2.withColumn(\"oldest_commit\", unix_timestamp(col(\"oldest_commit\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n\nactiveprojectsduration_temp.show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["As part of the data scope assessment, new fields for the oldest contribution date (oldest_date) and newest contribution date (newest_date) can be deduced from the existing columns of \"latest_pr\", \"oldest_pr\", \"latest_commit\", and \"oldest_commit\". Since all projects have either a set of contribution dates for PRs or commits - but not both - it is possible to find the oldest and newest contribution date by using the when function to determine whether or not a field is null and populate it with the remaining field. This is shown in the cell below."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions._\n\n// find oldest_date and newest_date between commit and pr dates\nval updatedDF3 = activeprojectsduration_temp.withColumn(\"oldest_date\", when($\"oldest_pr\".isNull, $\"oldest_commit\").otherwise($\"oldest_pr\"))\nval updatedDF4 = updatedDF3.withColumn(\"newest_date\", when($\"latest_pr\".isNull, $\"latest_commit\").otherwise($\"latest_pr\"))\n\nupdatedDF4.show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["To continue with the data scope assessment, a new dataframe is then created by performing the following:\n- Dropping the individual newest and oldest contribution dates for PRs and commits\n- Finding all projects with 2 or more members making contributions - if a project only has 1 active developer, then they are not collaborating with anyone else and therefore there are no social ties to investigate. A count of the developers grouped by project_id is performed and a filter condition is placed to include only projects with counts greater than or equal to 2 members. Dataframe then drops the counts column as the data is irrelevant to the analysis in the project"],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// display dataframe with oldest_contribution_date and newest_contribution_date\nval updatedDF5 = updatedDF4.selectExpr(\"project_id\", \"project_name\", \"member_id\", \"member_login\", \"oldest_date as oldest_contribution_date\", \"newest_date as newest_contribution_date\")\n\n// find all projects with greater than 2 members making contributions\nval updatedDF6 = updatedDF5.groupBy(updatedDF(\"project_id\")).count.withColumnRenamed(\"count\", \"n\").filter(\"n >= 2\")\n\n// filter dataframe of all projects using list of projects that have 2+ members\nval activeprojectsduration = updatedDF5.join(updatedDF6, \"project_id\").drop(\"n\")\n\nactiveprojectsduration.show()\n\n// register this data as an SQL table\nactiveprojectsduration.createOrReplaceTempView(\"activeprojectsduration\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Place data in dataframe and observe\ndf = sqlContext.table(\"activeprojectsduration\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["##### Basic Questions to Assess Data Structure\n1. **Do all records in the dataset contain the same fields?**\n   According to the describe function above, all 222 records in the watchersweekly dataset have the repo_id, number_watchers, and week_number field.\n2. **How can you access the same fields across records? By position? By name?**\n   Using the name of the column will allow access of the same fields across records.\n3. **How are the records delimited/separated in the dataset? Do you need sophisticated parsing logic to separate the records from one another?**\n   Records are delimited using a comma \",\". Sophisticated parsing logic is not required in order to separate the records.\n4. **How are record fields encoded? Human readable strings? Binary numbers? Hash keys? Compressed? Enumerated codes?**\n   The record fields are encoded using human readable strings."],"metadata":{}},{"cell_type":"markdown","source":["##### Basic Questions to Assess Data Granularity\n- **What kind of thing (person, object, relationship, event, etc.) do the records represent?** \n  The records represent the total weekly number of \"events\" - the number of users who have starred a repository in a given week.\n- **Are the records homogeneous (represent the same kinds of things)? Or heterogeneous?** \n    The records are homogeneous - they all show the weekly count of star events (up to 52 weeks) for each repository.\n- **What alternative interpretations of the records are there? For example, if the records appear to be customers, could they actually be all known contacts (only some of which are customers)?** \n  Alternative interpretations of the records could be that the events actually represent the \"watch\" event; however, according to Gousios [6], the watchers events can be used interchangably with the stars events."],"metadata":{}},{"cell_type":"markdown","source":["##### Basic Questions to Assess Data Accuracy\n- **For date times, are time zones included or adjusted into a standard time zone like UTC?** \n  Yes, timestamps are converted to UTC (see created_at fields). However, according to Gousios [6], projects or events which had null values for the created_at field were replaced with the latest date that the corresponding user or project has been created. \n- **What is the measurable distribution of inaccuracies?** Due to the fact that the timestamps have been prepopulated, it is difficult to determine whether the original values were null and therefore inaccurate.\n\nFor questions pertaining to addresses, names, email addresses, numeric items like phone numbers and UPC codes, and sales transactions - these data accuracies do not apply as the Github dataset that is required for the project analysis does not use these variables. Questions pertaining to data input by sensors or manual/human input are also not relevant to the Github dataset."],"metadata":{}},{"cell_type":"markdown","source":["##### Basic Questions to Assess Data Temporality\n- **When was the dataset collected?** \n  Dataset was collected from September 1 2017 (from ghtorrent-bq:ght_2017_09_01 on BigQuery)\n- **Were all the records and record fields collected/measured at the same time? If not, is the temporal range significant?** Records were not measured at the same time. The temporal range is quite significant, with 5 years in between the earliest and latest timestamps in the created_at column.\n- **Are the timestamps associated with collection of the data known and available (as a record field) or as associated metadata?** Yes - see created_at columns\n- **Have some records or record field values been modified after the time of creation? Are the timestamps of these modifications available?** Yes - see updated_at columns\n- **In what ways can you determine if the data is \"stale\"? For example, you might have purchased a marketing leads databased and want to verify the contact information for the people represented in the dataset. Is it sufficient to sample the records and manually verify the data? Can you automatically verify it by using third-party services?** Can determine whether or not data is stale by manually comparing the repositories in the dataset to the repositories available to Github. However, this cannot be automically verified using a third-party service.\n- **If there are conflicting values in the data (i.e. multiple mailing addresses for a person), can you use timestamps to determine which value is \"correct\"** If there are conflicting values in the data, it may be difficult to use timestamps to determine which value is \"correct\" due to the fact that the created_at field is only filled in accurately for starrings for which GHTorrent has recorded a corresponding event - otherwise, it is filled in with the latest date that the corresponding user or project has been created."],"metadata":{}},{"cell_type":"markdown","source":["##### Basic Questions to Assess Data Scope\n- **Given the granularity of the dataset, what characteristics of the things (i.e. people, objects, relationships, events, etc.) represented by the records are captured by the record fields? What characteristics are not captured?**  Events (when a member stars a repository) and relationships (developers working on a particular repository) are represented by the record fields. \n- **Are the record fields consistent? For example, does the customer's age field make sense relative to the date-of-birth field? If the record corresponds to a purchase transaction, does the cost of the listed set of purchased items add up to the total transaction amount?**  Fields are consistent - created_at and updated_at \n- **For the analysis you want to perform, can you deduce or infer additional relevant characteristics from the ones that you want? For example, can you infer the demographics of the people in a household from partner and dependents record fields?** Can infer when developers are working together on the same project at the same time by looking at their first and last commit dates. If time frames overlap, then they are collaborators/working on the same project at the same time.\n- **Are the same record fields available for all records? Are they accessible via the same specification (position, name, etc.)?** The same record fields are available for all records."],"metadata":{}},{"cell_type":"markdown","source":["#### Modelling\n\nThe modelling section of the report will focus on implementation of two models - first, using Graph X to model the developer network; and second, using k-means clustering on the amount of stars a project has to determine the popularity of the projects."],"metadata":{}},{"cell_type":"markdown","source":["#### Graph X Implementation\n\nAs outlined in the methodology by Antwerp et al. [3], two graphs will be created in order to determine the developer network. - a developer-project bipartite graph, and a developer-developer graph."],"metadata":{}},{"cell_type":"code","source":["%scala \n\n// import required packages\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.ml.param.ParamMap"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["#### Developer-Project Bipartite Graph\n\nAccording to Antwerp et al. [4], a developer-project bipartite graph was created first to determine the relationships between projects and the developers who work on said projects. The vertices will be represented by both developers and projects, and an edge will be created if a developer has worked on a project. However, due to the fact that data preprocessing can be used to generate the data required to create the developer-developer network, this approach will be taken instead of building a developer-project bipartite graph first, then creating a developer-developer graph from the bipartite graph. As a result, the cell below for the bipartite graph is just a reference."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nclass VertexProperty extends Serializable {}\n\ncase class ProjectProperty(val id: Int, val name: String) extends VertexProperty\ncase class MemberProperty(val id: Int, val name: String) extends VertexProperty\n\nval project: RDD[(VertexId, VertexProperty)] = activeprojectsduration.rdd.map(row => (row(0).asInstanceOf[Number].longValue, row(1).asInstanceOf[VertexProperty]))\nval member: RDD[(VertexId, VertexProperty)] = activeprojectsduration.rdd.map(row => (row(2).asInstanceOf[Number].longValue, row(3).asInstanceOf[VertexProperty]))\nval vertX = VertexRDD(project ++ member)\nval edgX: RDD[Edge[Int]] = activeprojectsduration.select(\"project_id\", \"member_id\").rdd.map(row => Edge(row(0).asInstanceOf[Number].longValue, row(1).asInstanceOf[Number].longValue))\n\nval bpgraph: Graph[VertexProperty, Int] = Graph(vertX, edgX)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["####Developer-Developer Graph\n\nNext, a developer-developer graph will be created to determine the ties between developers who have worked together. The vertices will be represented by developers, and an edge will be created if a developer has worked with another developer. The edges will also have an attribute which represents how many occurences a particular pair of developers have worked together. \n\nThe process of creating the developer-developer graph begins with encoding the project_id and member_id columns with 'p' and 'm' respectively. This is to ensure that if a project_id has the same value as a member_id, they will not erroneously match together. The resulting dataframe with the encoded ID values are stored in \"dfe\"."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// to prevent matching of the same value from project_id and member_id, project_id was encoded with 'p' and member_id was encoded with 'm' and placed in new dataframe\nval dfe = activeprojectsduration.select(\n  concat(lit(\"p\"), $\"project_id\").as(\"project_id\"),\n  $\"project_name\",\n  concat(lit(\"m\"), $\"member_id\").as(\"member_id\"),\n  $\"member_login\",\n  $\"oldest_contribution_date\",\n  $\"newest_contribution_date\"\n)\n\ndfe.show()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["Once the new dataframe with the encoded IDs are created, it is duplicated into two dataframes \"df1\" and \"df2\". The resulting \"df1\" and \"df2\" are joined with the condition that the project_ids are the same but the member_ids are not the same - this will result in the developer-developer pairs for each project but ensure that each developer-developer pair does not include the developer paired with themselves. Data scope assessment is then performed on the new dataframe \"ddpairs_initial\" to deduce a new column \"worked_together\". Based on whether or not the contribution date range of the 1st developer is within the contribution date range of the 2nd developer, the worked_together column will be populated with 1 (worked together on the same project within the same timeframe), or 0 (have not worked together on the same project within the same timeframe; in other words, worked on the same project BUT not within the same timeframe). \n\nIn order to create edges in the developer-developer graph, the edges must have a srcId and a dstId corresponding to the source and destination vertex identifiers, as well as an attr member which stores the edge property. Thus, the dataframe is grouped by the developer-developer pairs with each developer being either a \"srcId\" or \"dstId\" and the worked_together column is summed for each pair of developers to determine the total number of times that the pair worked together (stored in the \"attr\" column). The final dataframe of the developer-developer pairs and the number of times they have worked together is shown in \"dd\"."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nval df1 = dfe.select(\"project_id\", \"project_name\", \"member_id\", \"member_login\", \"oldest_contribution_date\", \"newest_contribution_date\")\nval df2 = dfe.select(\"project_id\", \"project_name\", \"member_id\", \"member_login\", \"oldest_contribution_date\", \"newest_contribution_date\")\n\n// join condition\nval joinCondition = when($\"df1.member_id\" =!= $\"df2.member_id\", $\"df1.project_id\" === $\"df2.project_id\")\n\n\n// find developer-developer pairs and its occurence (attr)\nval ddpairs_initial = df1.alias(\"df1\")\n  .join(df2.alias(\"df2\"), joinCondition).drop($\"df2.project_name\")\n\nval ddpairs = ddpairs_initial.withColumn(\"worked_together\", when($\"df1.oldest_contribution_date\" < $\"df2.newest_contribution_date\" and $\"df1.newest_contribution_date\" >= $\"df2.oldest_contribution_date\", 1).otherwise(0))\n  // sort \"source\" (developer 1) and \"destination\" (developer 2)\n  .groupBy(\n     greatest(\"df1.member_id\", \"df2.member_id\").as(\"srcId\"), \n     least(\"df1.member_id\", \"df2.member_id\").as(\"dstId\"))\n  .agg(sum($\"worked_together\").as(\"attr\"))\n\n\nval dd = ddpairs.selectExpr(\"srcId\",\n                      \"dstId\",\n                      \"attr\")\ndd.show()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["As mentioned previously, developer-developer pairs who have a 0 in the \"worked_together\" column will not result in an edge since they are not collaborators on a project (did not work on a project together at the same time). Therefore, they are filtered from the dataframe, which results in the new dataframe \"dd_filtered\"."],"metadata":{}},{"cell_type":"code","source":["%scala \n\nval dd_filtered = dd.filter(\"attr > 0\")\n\ndd_filtered.show()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["The \"srcId\" and \"distId\" columns of the \"dd_filtered\" dataframe is converted to integer values in preparation for the creation of the edges in the developer-developer graph. As mentioned previously, the edges of the graph are created when there is a working relationship (worked on a project together within the same timeframe) between two developers."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n//convert srcId and dstId to int\nval indexer = new StringIndexer().setInputCol(\"col\").fit(\n   dd_filtered.select(\"srcId\").toDF(\"col\").union(dd_filtered.select(\"dstId\").toDF(\"col\"))\n)\n\nval result = Seq(\"srcId\", \"dstId\").foldLeft(dd_filtered){\n  (dd_filtered, col) => indexer\n    .copy(new ParamMap()\n      .put(indexer.inputCol, col)\n      .put(indexer.outputCol, s\"${col}_idx\"))\n    .transform(dd_filtered)\n}\n\nval resultf = result.selectExpr(\"cast(srcId_idx as int) srcId_idx\", \"cast(dstId_idx as int) dstId_idx\", \"cast(attr as int) attr\")\nresultf.show()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["By calling the describe function on the \"resultf\" dataframe, it can be seen that within the developer-developer pairs, there was a minimum of 2 times which the pairs worked together; the most that a pair of developers worked together was 8 times."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nresultf.describe().show()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["The \"activeprojectsduration\" dataframe is filtered for distinct member IDs in order to return a list of members for the active projects. This resulting dataframe \"dfdist\" of the project members will be used to construct the nodes of the developer-developer graph."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nval dfdist = activeprojectsduration.selectExpr(\"member_id\", \"member_login\").distinct\ndfdist.show()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["As a note on data scope assessment, the dataset has a total of 7675 developers."],"metadata":{}},{"cell_type":"code","source":["%scala\n\ndfdist.describe().show()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["The following cell describes the developer-developer graph implementation. The nodes are first constructed using the dfdist dataframe and stored in \"distmember\". The edges are then constructed using the resultf dataframe and stored in \"edgeRDD\". Finally, the graph is created by calling the graph function and passing in \"distmember\" and \"edgeRDD\", and stored in \"ddgraph\". The number of verticies (nodes) in the graph are found to be 12532, which represents the 12532 developers. The number of edges in the graph are found to be 13598, which are the amount of ties between the 12532 developers. The number of triplets represent the 13598 developer-to-developer relationships and the attr property (in this case, how many times they have collaborated together) between them."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// define edges and vertices of developer-developer graph\nval distmember: RDD[(VertexId, String)] = dfdist.select(\"member_id\", \"member_login\").rdd.map(row => (row(0).asInstanceOf[Number].longValue, row(1).asInstanceOf[String]))\ndistmember.take(1)\n\nval edgeRDD = resultf.toDF(\"srcId\", \"dstId\", \"attr\").as[Edge[Int]].rdd\n\n// create developer-developer graph\nval ddgraph = Graph(distmember, edgeRDD)\nddgraph.cache()\n\nprintln(\"Total number of edges in the graph: \" + ddgraph.numEdges)\n//ddgraph.edges.take(10).foreach(println)\nprintln(\"Total number of vertices in the graph: \" + ddgraph.numVertices)\n\nprintln(\"Total number of triplets in the graph: \" + ddgraph.triplets.count)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["A common aggregation task is computing the degree of each vertex: the number of edges adjacent to each vertex. In the case of this analysis, it would present the number of developers that a particular developer has worked with. As shown in the cell below, developer with srcIdx of 0 worked with 165 other developers."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nval maxDegrees: (VertexId, Int) = ddgraph.degrees.reduce( (a,b) => if (a._2 > b._2) a else b)\nprintln(\"Developer who worked with the highest number of developers: \" + maxDegrees)\n"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Two simple queries back on the \"result\" and \"dfe\" tables (shown below) shows that it is the developer by the login of \"deerawan\" who has collaborated with 165 other developers."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nresult.select(\"srcId_idx\", \"srcId\", \"dstId_idx\", \"dstId\", \"attr\").where($\"srcId_idx\" === 0  or $\"dstId_idx\" === 0).show(1)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["%scala \n\ndfe.select(\"member_id\", \"member_login\").where($\"member_id\" === \"m1183849\").show()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Next, the connected components algorithm was used to identify each connected component of the developer-developer graph. This implementation labels each connected component of the developer-developer graph with the ID of the vertex with the lowest number. Identification of connected components is helpful due to the fact that it can be used to approximate clusters - from the results below, a value of 2 clusters could be appropriate. In undirected graphs such as the developer-developer graph, two vertices are connected if they have a path connecting them. In the case of a single vertex, such as the 7675 components with 1 vertex, it is still considered connected due to the reflexive property which means that any vertex is strongly connected to itself. The 1 connected component with 4857 vertices refers to a single connected component where there are 4857 developers connected within. It can be concluded that the resulting developer-developer graph is a connected graph - and since the graph is shown to be connected, it can be said that the developer-developer network is resilient."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport scala.collection.immutable.ListMap\n\n// Find connected components. Connected components algorithm labels each connected component of the graph with the ID of its lowest-numbered vertex. In a social network, connected components can approximate clusters.\nval ddgraphcc = ddgraph.connectedComponents()\n\nval ccNumVertices = \n  (ddgraph.vertices.map(pair => (pair._2,1))\n  .reduceByKey(_+_)  // count the number of vertices contained in each connected component (indexed by the smallest vertex index in the connected component)\n  .map(pair => pair._2)) // only maintain the number of vertices counted\nprintln(\"Number of Connected Components: \" + ccNumVertices.count)\n\nListMap(ccNumVertices.countByValue().toSeq.sortBy(_._1):_*).foreach(line => println(line._2 + \" connected component(s) with \" + line._1 + \" vertices\"));"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["Next, page rank algorithm was performed in order to determine the importance of each vertex (developer) in the developer-developer graph. In this case, if a developer has many ties with other developers,they will be ranked highly. The following cell returns the top 10 developers who have the most ties and therefore are the most signficant in the sense that the projects they participated in could be impacted by their abundance of their collaborator network."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// page rank on developer-developer-graph to find top 10 developers who are significant\nval ranks = ddgraph.pageRank(0.0001).vertices\nranks\n  .join(distmember)\n  .sortBy(_._2._1, ascending=false) // sort by the rank\n  .take(10) // get the top 10\n  .foreach(x => println(x._2._2))"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["The member IDs of top 10 developers with the greatest significance were used to filter the activeprojectsduration dataframe in order to determine which projects they had worked on. These projects will then be subject to a separate analysis on their project popularity in order to determine whether or not their popularity is indeed greater than the average project. The new dataframe \"td_report\" is created using the SQL query below."],"metadata":{}},{"cell_type":"code","source":["%scala \n\n//val topdevelopers: Array[String] = Array(stof, juanriaza, wong2, frewsxcv, JakeWharton, haikusw, nathenharvey, jbr, tenderlove, mmp)\n\n// find projects which the top developers have contributed to\nval td_report = activeprojectsduration.select(\"*\").where($\"member_login\" === \"stof\" or $\"member_login\" === \"juanriaza\" or $\"member_login\" === \"wong2\" or $\"member_login\" === \"frewsxcv\" or $\"member_login\" === \"JakeWharton\" or $\"member_login\" === \"haikusw\" or $\"member_login\" === \"nathenharvey\" or $\"member_login\" === \"jbr\" or $\"member_login\" === \"tenderlove\" or $\"member_login\" === \"mmp\")\n\ntd_report.show()\n\n//val topprojects = activeprojectsduration.select(\"project_id\" in td_report(\"project_id\"))"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["#### Watchers/Starred Events for Top Projects (identified from Developer-Developer Graph)\n\nIn order to determine whether developer networks impact repository popularity, the star events are taken from the projects which the top 10 developers with the most collaborators, and are compared with the general trends of the star events of all active projects. To begin, the watchers query shown above is filtered using a where clause in order to display only the repositories which have been identified as the projects worked on by the top 10 developers with the most collaborators (which was found using the page rank algorithm on the developer-developer graph)."],"metadata":{}},{"cell_type":"code","source":["'''\nQuery to return all watchers/starred events for top projects (identified from Developer-Developer graph)\n\nSELECT\n  t.repo_id AS repo_id,\n  COUNT(*) AS number_watchers,\n  t.week_number AS week_number\nFROM (\n  SELECT\n    repo_id,\n    created_at,\n    WEEK(created_at) AS week_number\n  FROM\n    [ghtorrent-bq:ght.watchers]\n  WHERE\n    repo_id = 39095969\n    OR repo_id = 17022118\n    OR repo_id = 42753525\n    OR repo_id = 4230805\n    OR repo_id = 43587\n    OR repo_id = 11474326\n    OR repo_id = 16827100\n    OR repo_id = 39780492\n    OR repo_id = 6261105\n    OR repo_id = 21316774\n  ORDER BY\n    repo_id,\n    week_number ) AS t\nGROUP BY\n  repo_id,\n  week_number\nORDER BY\n  repo_id,\n  week_number'''"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["Once imported, the topproj_watchersWeeklyRDD is then cleaned and registered as a SQL table."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Define schema\ncase class topproj_watchersWeekly(                 \n   repo_id: Int,\n   number_watchers: Int, \n   week_number: Int\n  )\n\n// Map data to schema\ndef topproj_cleanWatchersWeekly(row:Array[String]):topproj_watchersWeekly = {\nreturn topproj_watchersWeekly(\n    row(0).toIntOrElse(),\n    row(1).toIntOrElse(),\n    row(2).toIntOrElse()\n  )\n}\n\n// Create sql table \nval topproj_watchersweekly = topproj_watchersRDD.map(r => topproj_cleanWatchersWeekly(r)).toDF()\n\ntopproj_watchersweekly.show()\n\n// register this data as an SQL table\ntopproj_watchersweekly.createOrReplaceTempView(\"topproj_watchersweekly\")"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["#### Project Popularity Predictions\n\nAs outlined in the methodology by Antwerp et al. [3], popularity of the repositories will be performed by using clustering - first, the dataset is split into 2 to observe the popularity trends of the projects in the first year of project creation, and four years after project creation. Due to the fact that this analysis is looking at active projects, the Github dataset is split into half - the first 26 weeks (into \"dataset_fr\") to observe intial project popularity, and the remaining (into \"dataset_fr_2\") for the time until the one year mark will be used to observe the trends in project popularity. The fr_watchersDF is used since this dataframe was identified to have the full 53 week records of star events (as opposed to incomplete star events for weeks which may be missing). Each of the datasets are split into training and testing set to fit the k-means model and make predictions on the cluster, respectively. A pipeline is created to consist of the VectorAssembler and kmeans function. The k-value is chosen according to the results of the connected components algorithm performed earlier."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.Pipeline\n\n// load star data\nval dataset_fr = fr_watchersDF.select(\"repo_id\", \"number_watchers\", \"week_number\").where($\"week_number\" < 26)\nval dataset_fr_2 = fr_watchersDF.select(\"repo_id\", \"number_watchers\", \"week_number\").where($\"week_number\" >= 26)\n\n// split dataset into testing and training set\nval split_fr = dataset_fr.randomSplit(Array(0.7, 0.3), seed = 15L)\nval train_fr = split_fr(0).cache()\nval test_fr = split_fr(1).cache()\n\nval split_fr_2 = dataset_fr_2.randomSplit(Array(0.7, 0.3), seed = 15L)\nval train_fr_2 = split_fr_2(0).cache()\nval test_fr_2 = split_fr_2(1).cache()\n\n//creating features column\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"repo_id\", \"number_watchers\", \"week_number\"))\n  .setOutputCol(\"features\")\n\nval kmeans = new KMeans()\n  .setK(3)\n  .setSeed(1L)\n  .setFeaturesCol(\"features\")            // setting features column\n  .setPredictionCol(\"prediction\")       // setting label column\n\n//creating pipeline\nval pipeline = new Pipeline().setStages(Array(assembler,kmeans))\n\n//fitting the model\nval kMeansPredictionModel_fr = pipeline.fit(train_fr)\nval kMeansPredictionModel_fr_2 = pipeline.fit(train_fr_2)\n\n// make predictions\nval predictionResult_fr = kMeansPredictionModel_fr.transform(test_fr)\nval predictionResult_fr_2 = kMeansPredictionModel_fr_2.transform(test_fr_2)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["The first cell below shows the prediction results of the number of stars for the first 26 weeks of a project. The second cell below shows the prediction results of the number of stars for the remaining time of the year timeframe. When comparing the number_of_watchers v.s. week_number graphs (leftmost grid in the middle row) of the two cells, it is observed that the general trend for projects is that popularity starts of higher and decreases over time.\n\nThe k-value here is 3 and as shown in the graphs below, the clusters are as follows:\n- green represents the projects with the lowest popularity (have the least number of stars in a given week)\n- orange represents the projects with medium popularity (the number of stars in a given week are in the middle)\n- blue represents the projects with highest popularity (have the most number of stars in a given week)"],"metadata":{}},{"cell_type":"code","source":["%scala\n\ndisplay(predictionResult_fr)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\n\ndisplay(predictionResult_fr_2)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["Now a comparison is made with the popularity of the projects which top developers with the most connections to other developers are performed. This begins with splitting the dataset into first 26 weeks, and the remaining time for the second set; as well as creating training and testing data. The k-value is kept the same as the previous k-means model."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// load star data\nval tp_dataset = topproj_watchersweekly.select(\"*\").where($\"week_number\" < 26)\nval tp_dataset_2 = topproj_watchersweekly.select(\"*\").where($\"week_number\" >= 26)\n\n// split dataset into testing and training set\nval tp_split = tp_dataset.randomSplit(Array(0.7, 0.3), seed = 15L)\nval tp_train = tp_split(0).cache()\nval tp_test = tp_split(1).cache()\n\nval tp_split_2 = tp_dataset_2.randomSplit(Array(0.7, 0.3), seed = 15L)\nval tp_train_2 = tp_split_2(0).cache()\nval tp_test_2 = tp_split_2(1).cache()\n\nval kmeans_tp = new KMeans()\n  .setK(4)\n  .setSeed(1L)\n  .setFeaturesCol(\"features\")            // setting features column\n  .setPredictionCol(\"prediction\")       // setting label column\n\n//creating pipeline\nval pipeline_tp = new Pipeline().setStages(Array(assembler,kmeans_tp))\n\n//fitting the model\nval kMeansPredictionModel_tp = pipeline_tp.fit(tp_train)\nval kMeansPredictionModel_tp_2 = pipeline_tp.fit(tp_train_2)\n\n// make predictions\nval predictionResult_tp = kMeansPredictionModel_tp.transform(tp_test)\nval predictionResult_tp_2 = kMeansPredictionModel_tp_2.transform(tp_test_2)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"markdown","source":["The first cell below shows the prediction results of the number of stars for the first 26 weeks of the projects that top developers work on. The second cell below shows the prediction results of the number of stars for the remaining time of the year timeframe. When comparing the number_of_watchers v.s. week_number graphs (leftmost grid in the middle row) of the two cells, it is observed that the trend for projects which have developers who are highly significant (have a large collaborator network), is that the popularity rises with time. This contrasts sharply with the trend of decreasing popularity over time in the general projects.\n\nThe k-value here is 4 and as shown in the graphs below, the clusters are as follows:\n- green represents the projects with the lowest popularity (have the least number of stars in a given week)\n- blue represents the projects which are medium-low in terms of popularity (have the 2nd least number of stars in a given week)\n- red represents the projects which are medium-high in terms of popularity (have the 2nd most number of stars in a given week)\n- orange represents the projects with highest popularity (have the most number of stars in a given week)"],"metadata":{}},{"cell_type":"code","source":["%scala\n\ndisplay(predictionResult_tp)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%scala\n\ndisplay(predictionResult_tp_2)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["##### Evaluation - K Means\n\nSilhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot is a range from -1 to +1 and is a measure of how close each point in one cluster is to points in the neighboring clusters. It is used to assess parameters such as number of clusters visually. Values close to +1 indicate that the sample is far away from the neighboring clusters.\n\nThe silhouette score is calculated for each of the prediction results. As shown in below, the silhouette scores for the fr_watchersDF and topproj_watchersweekly prediction results are around the 0.72-0.82 range, which is relatively high and therefore a good indication that the chosen k-value is sufficient in creating distinct clustering of the datapoints. The cluster centers can be seen in the cells below as well."],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.ml.evaluation\nimport org.apache.spark.ml.evaluation.Evaluator\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator\n\n// evaluate clustering by computing silhouette score\nval evaluator = new ClusteringEvaluator()\n\nval silhouette_fr = evaluator.evaluate(predictionResult_fr)\nprintln(\"Silhouette with squared euclidean distance (FR): \" + silhouette_fr)\n\nprintln(\"Cluster Centers (FR): \")\nkMeansPredictionModel_fr.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)\n\nval silhouette_fr_2 = evaluator.evaluate(predictionResult_fr_2)\nprintln(\"Silhouette with squared euclidean distance (FR2): \" + silhouette_fr_2)\n\nprintln(\"Cluster Centers (FR2): \")\nkMeansPredictionModel_fr_2.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%scala\n\nval silhouette_tp = evaluator.evaluate(predictionResult_tp)\nprintln(\"Silhouette with squared euclidean distance (TP): \" + silhouette_tp)\n\nprintln(\"Cluster Centers (TP): \")\nkMeansPredictionModel_tp.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)\n\nval silhouette_tp_2 = evaluator.evaluate(predictionResult_tp_2)\nprintln(\"Silhouette with squared euclidean distance (TP2): \" + silhouette_tp_2)\n\nprintln(\"Cluster Centers (TP2): \")\nkMeansPredictionModel_tp_2.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["#### Summary of Results\n\nThe analysis in the modelling section of the report showed that:\n- The developer-developer network for active projects on GitHub is connected as evidenced by the connect components algorithm and therefore can be seen as a resilient network\n- Developer pairs collaborated together a minimum of 2 times and a maximum of 8 times on different projects\n- The maximum number of developers one developer worked with was 165 times\n- The general trend of project popularity starts off high but decreases as time goes by\n- The projects which are worked on by the top 10 developers who are most connected with other developers in the network, show increasing project popularity trends over time. This contrasts sharply with the general trend where project popularity decreases over time\n\nIn conclusion, it can be said that prior developer collaboration on projects does increase project popularity that is measured by the number of stars a project receives, as it allows relationships to foster between developers and to provide a greater understanding of each other's working habits as well as goals."],"metadata":{}},{"cell_type":"markdown","source":["#### Review / Future Work\n\nUpon review on the current analysis, the following steps were identified as possible areas to review:\n- Increasing the size of the datasets - once the data preparation of cleaning and removing invalid data, the final dataset had 13598 developers which is a small portion of the Github data. A larger dataset could be useful in providing concretenidentification of project popularity.\n- Including comments and watchers to the popularity measures as the current measure of stars only may not be able to capture the essence of a project's popularity by using a single method.\n- As outlined by the Github data study performed by Kalliamvakou et al. [8]:\n  * The number of commiters should have been accounted for in order to filter out the personal projects which may have appeared in the list of active projects\n  * Should not have relied on the status being merged for the pull request dataset (filter was applied in SQL queries run on BigQuery) as many PRs appear are non-merged even though they are and considered using heuristics instead"],"metadata":{}},{"cell_type":"markdown","source":["#### Bibliography\n\n[1] J. Yang and J. Leskovec, “Patterns of Temporal Variation in Online Media,” in *Proceedings of the Fourth ACM International Conference on Web Search and Data Mining*, New York, NY, USA, 2011, pp. 177–186.\n\n[2] J. Hahn, J. Y. Moon, and C. Zhang, “Impact of Social Ties on Open Source Project Team Formation,” in *Open Source Systems*, 2006, pp. 307–317.\n\n[3] M. V. Antwerp and G. Madey, “The Importance of Social Network Structure in the Open Source Software Developer Community,” in *2010 43rd Hawaii International Conference on System Sciences*, 2010, pp. 1–10.\n\n[4] H. Borges, A. Hora, and M. T. Valente, “Understanding the Factors That Impact the Popularity of GitHub Repositories,” in *2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)*, 2016, pp. 334–344.\n\n[5] H. Borges, A. Hora, and M. T. Valente, “Predicting the Popularity of GitHub Repositories,” in *Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering*, New York, NY, USA, 2016, pp. 9:1–9:10.\n\n[6] G. Gousios, \"The GHTorrent dataset and tool suite\" in *Proceedings of the 10th Working Conference on Mining Software Repositories (MSR)*, San Francisco, CA, USA, 2013, pp. 233-236\n\n[7] T. Rattenbury, J. M. Hellerstein, J. Heer, C. Carreras, and S. Kandel, Principles of Data Wrangling: Practical Techniques for Data Preparation. O'Reilly Media, 2017.\n\n[8] E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. German, and D. Damian, “The Promises and Perils of Mining GitHub,” in Proceedings of the 11th Working Conference on Mining Software Repositories, New York, NY, USA, 2014, pp. 92–101."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":110}],"metadata":{"name":"Lo-Tracy-GitHubSocialTies-Final-Databricks","notebookId":1709521144698643},"nbformat":4,"nbformat_minor":0}
